
<head>
<link href='http://fonts.googleapis.com/css?family=Roboto&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="css/reset.css" />
<link rel="stylesheet" href="css/beijbom_web.css" />

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
</head>
<body class="show" id="pages">
<script src="http://code.jquery.com/jquery-1.9.1.js" type="text/javascript"></script>


<div class="center">
<a class="section-anchor section-anchor-top" data-scroll-name="#main"></a>
<h1>Oscar Beijbom</h1> 
<p class="abstract"><a href="mailto:obeijbom@eecs.berkeley.edu">obeijbom@eecs.berkeley.edu</a></p>
	

<p class="abstract">I'm doing a post-doc at the <a href="http://bvlc.eecs.berkeley.edu/">Berkeley Vision and Learning Center</a>, where I work on automated quantification of scientific image-data using deep learning.  My work is jointly supervised by <a href="https://www.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> at UC Berkeley and <a href="http://researchers.uq.edu.au/researcher/839">Ove Hoegh-Guldberg</a> at University of Queensland. 

<p class="abstract">Before this, I studied computer vision and machine learning at <a href="http://vision.ucsd.edu/">UCSD</a> under <a href="http://cseweb.ucsd.edu/~kriegman/">David Kriegman</a> and <a href="http://vision.cornell.edu/se3/people/serge-belongie/">Serge Belongie</a>, and engineering physics at <a href="http://www.lth.se/english/">Lund University</a> under <a href="http://www2.maths.lth.se/vision/">Kalle Åström</a>.

<p class="abstract">Outside academia, I was lead developer at <a href="http://hovding.com">Hövding</a> where I created the algorithmic framework and hardware design for their <a href="http://techcrunch.com/2012/08/15/invisible-bike-helmet/">invisible bicycle helmet</a>. I have also worked on <a href="#menu-match">automated dietary logging systems</a> for consumer applications and <a href="#cellavision">focusing algorithms</a> for image-based cell analysis.</p>

<p class="abstract">Lately I have been having tons of fun developing <a href="http://coralnet.ucsd.edu">CoralNet</a>, deploying deep convolutional neural networks to help coral reef ecologists mine image data.</p> 

<ul class="social">
<li class="first-item"><a href="#projects">Projects</a></li>
<li><a href="#pubs">Publications</a></li>
<li><a href="#press">Press</a></li>
<li><a href="#data">Data</a></li>
<li><a href="https://www.linkedin.com/in/oscar-beijbom-696aa6">LinkedIn</a></li>
</ul>

<p class="uparrow"></p>

<a class="section-anchor" data-scroll-name="#projects"></a>
<h2>Projects</h2>
<p> Some of my favorite previous and current research projects.</p>

	<a name="coralnet"> </a>
	<h3>Mapping the Worlds Coral Reefs</h3>
		<p><em>UCSD, San Diego, California</em></p>	
		<p><a href="http://coralnet.ucsd.edu"><img src="images/coralreef.jpg" alt="CoralNet" width="200" height="130"></a>With the world's coral reef in unprecedented decline, much effort is focused on research, restoration, and conservation. Towards these ends, ecological composition across large spatial and temporal scales needs to be measured. Fortunately, progress in digital imagery and underwater robotics allow for rapid <a href="http://catlinseaviewsurvey.com/">collection of large image corpora</a>. Unfortunately, however, subsequent manual image analysis is <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130312">prohibitively time consuming</a> creating a "manual annotation bottleneck" between collected images and tabulated data.</p>

		<p>Addressing this bottleneck was the over-arching goal of my PhD, and culminated with the creation of <a href="coralnet.ucsd.edu">CoralNet</a>. CoralNet implements the <a href="http://vision.ucsd.edu/sites/default/files/automated_coral_annotation.pdf">computer vision methods</a> that I developed, and allows researchers, agencies, and private partners to processed their survey images in several automated and semi-automated annotation modes. The site also serves as a repository and collaboration platform for the scientific community. Current users include the <a href="http://www.pifsc.noaa.gov/cred/">NOAA Coral Reef Ecosystem Program</a>, the <a href="http://www.catlinseaviewsurvey.com/">Catlin Seaview Survey</a>, the <a href="http://www.aims.gov.au/">the <a href="http://www.aims.gov.au/">Australian Institute of Marine Science</a>, Washington State University, University of Washington, University of North Carolina, Scripps Institution of Oceanography, Colby College</a>, and <a href="http://stanford.sea.edu/">Stanford@Sea</a>. Photo credit: Catlin Seaview Survey </p>

	<h3>Learning to Count</h3>
		<p><em>UC Berkeley, California</em></p>
		<p><a href="http://arxiv.org/pdf/1510.04811v1.pdf"><img src="images/balls.jpg" width="200" height="150"></a>
		Quantifying the content of a single image, or a set of images is important in many practical situation. Visual sampling surveys, for example, which is used throughout the natural sciences, require just this. With this in mind, there is surprisingly little work on this topic in computer vision, as it falls outside the standard tasks of classification, segmentation or detection.</p>
		<p>I have looked at a few aspects of this problem. For example: how do you best integrate an automated annotation system when designing a <a href="http://arxiv.org/pdf/1410.7074v4.pdf">random sampling survey</a>, or what is the optimal way to quantify the <a href="http://arxiv.org/pdf/1510.04811v1.pdf">class-distribution</a> under <a href="http://arxiv.org/abs/1211.4860">domain-shift</a> given a finite set of images?</p>	


	<h3>Classifying with Costs</h3>
		<p><em>UCSD, San Diego, California</em></p>
		<p><a href="http://vision.ucsd.edu/sites/default/files/guess_averse.pdf"><img src="images/tree.jpg" alt="guess_averse" width="200" height="150"></a>
		Supervised machine learning algorithms are typically studied for cost-balanced binary classification, where the goal is to separate two equally important classes. However, in many practical situations, there are multiple classes, and in addition, misclassification costs may vary. This leads to the more challenging problem of cost-sensitive multiclass learning. </p>
		<p>I worked with <a href="vision.caltech.edu/~sbranson">Steve Branson</a> on a structured<a href="http://vision.ucsd.edu/sites/default/files/cvpr2013_efficient_0.pdf"> Support Vector Machine</a> solver for cost-sensitive multiclass learning that converges orders of magnitudes faster than previous solvers. In another project I examined loss functions for cost-sensitive multiclass learning together with <a href="http://www.svcl.ucsd.edu/~ehsan/">Mohammad Saberian</a>, and identified a property, <a href="http://vision.ucsd.edu/sites/default/files/guess_averse.pdf">guess-averseness</a>, which has strong empirical importance.</p>


	<h3>Counting Calories</h3>
		<a name="menu-match"></a>
		<p><em>Microsoft Research, Seattle, Washington, USA</em></p>
    	<p><a href="http://research.microsoft.com/menumatch/data/"> <img src="images/menu-match.jpg" alt="menu-match" width="200" height="150"></a>The World Health Organization (WHO) predicts that overweight and obesity may soon replace more traditional public health concerns such as under-nutrition and infectious diseases as the most significant cause of poor health. Logging food and calorie intake has been shown to facilitate weight management, but current food logging methods are time-consuming and cumbersome, which limits their effectiveness. </p>
		<p>During an internship at <a href="http://research.microsoft.com/en-us/">Microsoft Research</a>, I worked with <a href="http://research.microsoft.com/en-us/um/people/neel/index.html">Neel Joshi</a> and <a href="http://research.microsoft.com/en-us/um/people/dan/">Dan Morris</a>, to develop a <a href="https://www.dropbox.com/s/xk4tq94r9r3c2zm/menu-match.pdf?dl=1">practical method</a> for food-logging from images. The method utilize a data-base of menu items to estimate the nutritional content of the query image, and we demonstrate robust performance on a dataset of <a href="http://research.microsoft.com/menumatch/data/">realistic food images</a>.

	
    <h3>Building An Invisible Bike Helmet</h3>
    	<a name="hovding"> </a>
    	<p><em>Hövding AB, Malmö Sweden</em></p>
    	<p><a href="http://www.hovding.com"><img src="images/helmet_model.jpg" alt="hövding" width="200" height="300"></a> From <a href="http://www.hovding.com">Hövding's website</a>: <em>"Hövding is a bicycle helmet unlike any other currently on the market. It's ergonomic, it's practical, it complies with all the safety requirements, and it's also subtle and blends in with what else you are wearing. Hövding is a collar for bicyclists, worn around the neck. The collar contains a folded up airbag that you'll only see if you happen to have an accident. The airbag is shaped like a hood, surrounding and protecting the bicyclist's head. The trigger mechanism is controlled by sensors which pick up the abnormal movements of a bicyclist in an accident. Hövding is a practical accessory that's easy to carry around, it's got a great-looking yet subtle design, and it will save your life."</em> </p>
                
    	<p> I was fortunate to be the first employee at Hövding where my task was to develop the accident detection system from scratch. This involved everything from selecting hardware sensors, collecting train data, setting up a computational infrastructure, defining target performances, and developing the actual algorithm. I worked with a small team of engineers on this task. </p>
                
    	<p> Hövding has been covered in thousands of video and news-releases, including stories from <a href="http://www.nbcnews.com/tech/innovation/invisible-bicycle-helmet-airbag-head-f2D11599972">NBC</a>, <a href="http://www.wired.co.uk/news/archive/2010-10/21/cyclist-collar-airbag-helmet">WIRED</a>,
		<a href="http://voices.washingtonpost.com/dr-gridlock/2010/10/airbag_for_cyclists_unveiled.html">The Washington Post</a>, and <a href="http://www.spiegel.de/reise/deutschland/fahrradhelm-so-findet-man-sichere-und-schoene-modelle-a-897161.html">der Spiegel</a>.  Filmmaker Fredrik Gerttens <a href="https://player.vimeo.com/video/43038579">short-film</a> about Hövding have been viewed over 20 million times. Some of my other personal favorites are <a href="https://www.youtube.com/watch?v=TnmJISC1KVw">this story</a> by a Swiss independent reporter, and <a href="http://tvtotal.prosieben.de/tvtotal/videos/player/index.html?contentId=152524&initialTab=related">this one</a> by a German talk show host. Please see <a href="http://www.hovding.com/press/">Hövdings</a> press page for a complete list.</p>


	<h3>Blood cell focus</h3>
		<a name="cellavision"></a>
    	<p><em>Cellavision AB, Lund Sweden</em></p>
        <p><a href="http://www.cellavision.com"><img src="images/cells.jpg" alt="celsl" width="200" height="200"></a>From <a href="http://www.cellavision.com">Cellavisions website</a>: <em>"CellaVision develops and markets products for the health care sector, enabling fast and firm blood cell analysis and quality assurance of morphology diagnosis. The company has cutting-edge expertise within sophisticated digital image analysis, artificial intelligence and automated microscopy. For laboratories, this means increased efficiency, a simplification of the procedures and confirmed proficiency. The product line includes systems for automatic blood cell differentials and software for differential proficiency testing and education. The products are sold to hospitals and laboratories in Europe, North America and Asia."</em> </p>
                
        <p> I did my masters thesis at Cellavision, where I worked on a single-image focus level assessment method for blood cell images. This is in contrast with standard methods for image focusing that use multiple images together to determine focus level. The method I developed has an <a href="http://www.google.com/patents/US20100177187">international patent</a> and is currently deployed in Cellavision products. </p>
           
<p class="uparrow border"><a href="#main">&#9650</a></p>


<a class="section-anchor" data-scroll-name="#pubs"></a>
<h2>Selected publications</h2>
<p> Full list of publications is available at <a href="http://scholar.google.com/citations?user=XP_Hxm4AAAAJ&hl=en">Google Scholar profile</a></p>
<ul>
	<li>O. Beijbom et al. <i>"Towards automated annotation of benthic survey images: Variability of human experts and operational modes of automation"</i>. PLoS One, 2015.
		[<a href="https://www.dropbox.com/s/upy5kkl4qxla036/ios_plos2015.pdf?dl=1"]>pdf</a>]
		[<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130312">www</a>]
		[<a href="http://datadryad.org/resource/doi:10.5061/dryad.m5pr3">data</a>]
	</li>	

    <li >O. Beijbom, N. Joshi, D. Morris, S. Saponas, S. Khullar. <i>"Menu-Match: Restaurant-Specific Food Logging from Images"</i>. Winter Conference on Applications of Computer Vision (WACV), 2015. 
		[<a href="https://www.dropbox.com/s/xk4tq94r9r3c2zm/menu-match.pdf?dl=0">paper</a>]
		[<a href="http://research.microsoft.com/menumatch/data/">data</a>]
		[<a href="http://research.microsoft.com/menumatch/">www</a>]
	</li>

	<li >O. Beijbom<sup>*</sup>, M. Saberian<sup>*</sup>, N. Vasconcelos, D. Kriegman. <i>"Guess Averse Loss Functions for Cost-Sensitive Multiclass Boosting"</i>. International Conference on Machine Learning (ICML), Beijing, June 2014. <sup>*</sup>equal contribution. 
		[<a href="http://vision.ucsd.edu/sites/default/files/guess_averse.pdf">paper</a>] 
        [<a href="http://vision.ucsd.edu/sites/default/files/guess_averse_supplementary_0.pdf">supp. info</a>] 
        [<a href="https://www.dropbox.com/s/v5q7rrbwf4bc50i/guess_averse_icml_talk_beijbom.pptx">talk (ppt)</a>]
        [<a href="http://techtalks.tv/talks/guess-averse-loss-functions-for-cost-sensitive-multiclass-boosting/60880/">talk (video)</a>] 
        [<a href="https://www.dropbox.com/s/liv681yzcaw5hnb/guess_averse_poster.pptx">poster</a>] 
        [<a href="http://vision.ucsd.edu/~beijbom/files/ICML_2014_guess_averse_code_data.zip">code</a>]
    </li>

    <li >S. Branson, O. Beijbom, S. Belongie. <i> "Efficient Large-Scale Structured Learning"</i>. IEEE Conference on Computer Vision (CVPR), Portland, Oregon, June 2013.
		[<a href="http://vision.ucsd.edu/sites/default/files/cvpr2013_efficient_0.pdf">paper</a>]
		[<a href="http://vision.caltech.edu/~sbranson/files/branson2013efficient.pptx">talk (ppt)</a>]
		[<a href="http://techtalks.tv/talks/efficient-large-scale-structured-learning/58645/">talk (video)</a>]
		[<a href="http://vision.caltech.edu/~sbranson/code/index.html#learning">code</a>]
	</li>

	<li >O. Beijbom, P.J.Edmunds, D.I.Kline, B.G.Mitchell, D.Kriegman. <i>"Automated Annotation of Coral Reef Survey Images"</i>. IEEE Conference on Computer Vision (CVPR), Providence, Rhode Island, June 2012.
		[<a href="http://vision.ucsd.edu/sites/default/files/automated_coral_annotation.pdf">paper</a>]
		[<a href="http://vision.ucsd.edu/content/moorea-labeled-corals">data</a>]
	</li>
</ul>  

<p class="uparrow border"><a href="#main">&#9650</a></p>


<a class="section-anchor" data-scroll-name="#press"></a>
<h2>Press</h2>
<ul >
	<li >Mar 2015: Our <a href="http://www.jacobsschool.ucsd.edu/re/">UCSD research expo</a> poster highlighted on the UCSD Jacobs School of Engineering blog. [<a href = "http://jacobsschoolofengineering.blogspot.com/2015/03/meet-engineers-of-tomorrow-oscar-beijbom.html">www</a>]</li>
	
	<li >Nov 2014: Jonathan Cohen at <a href="http://www.nvidia.com/content/global/global.php">NVIDIA</a> highlights <a href="projects.html#coralnet">CoralNet</a> in his talk at the SuperComputer conference. CoralNet section starts at 9.30. [<a href="http://on-demand.gputechconf.com/supercomputing/2014/video/SC411-machine-learning-computational-researchers.html">www</a>]</li>

	<li >May 2014: Destin at <a href="https://www.youtube.com/user/destinws2">SmarterEveryDay</a> follows the data collected by the <a href="http://catlinseaviewsurvey.com/">Catlin Seaview Survey</a> all the way to <a href="projects.html#coralnet">CoralNet</a>. [<a href="https://www.youtube.com/watch?v=az1PTIehYKI">www</a>]</li> 
			
	<li >November 2013: NBCnews does a nice story on <a href="projects.html#hovding">Hövding</a>. [<a href="http://www.nbcnews.com/tech/innovation/invisible-bicycle-helmet-airbag-head-f2D11599972">www</a>]</li> 			
			
	<li >September 2013: Greenwire covers <a href="projects.html#coralnet">CoralNet</a>. [<a href="http://www.eenews.net/greenwire/stories/1059986651">www</a>]</li>

	<li >April 2012: German TV talk-show host tries to outsmart the accident detection system of <a href="projects.html#hovding">Hövding</a>. Check out 05:45 where a stunt man stages an accident and Hövding inflates, and 07:10 - 8:00 where the host tries to trick Hövding into inflating when it shouldn't! [<a href="http://tvtotal.prosieben.de/tvtotal/videos/player/index.html?contentId=152524&initialTab=related">www</a>]</li> 
	
	<li >May 2012: Fredrik Gerttens creates a short-film about the founders of <a href="projects.html#hovding">Hövding</a>. [<a href="https://player.vimeo.com/video/43038579">www</a>]</li> 
</ul>

<p class="uparrow border"><a href="#main">&#9650</a></p>


<a class="section-anchor" data-scroll-name="#data"></a>
<h2>Data</h2>
<p>Below are links to some data-sets that I have packaged or published</p>
	
	<h3>Quantification data-sets</h3>
		<p class="margin_top"><a href="quantification_dataset.html"><img src="images/css_class_counts_legend_small.jpg" alt="" width="180" height="130"></a>This data-set is an aggregate of two marine ecology data-sets packaged for quantification and domain adaptation experiments. One data-set is a Caribbean coral reef survey by the <a href="http://www.catlinseaviewsurvey.com">Catlin Seaview Survey</a>, and one a plankton time-series from <a href="http://www.whoi.edu/oceanus/feature/building-an-automated-underwater-microscope">Martha's Vineyard Coastal Observatory</a>. The two data-sets, while superficially different, pose the same question, namely: how can we quantify the label-distribution across multiple repeated data-set shifts at the lowest possible manual annotation effort? 

		This data-set and the quantification challenge are described in <a href=http://arxiv.org/abs/1510.04811>this arXiv preprint</a>, which in turn reference the original publications.</p>


	<h3>Pacific Labeled Corals</h3>
		<p class="margin_top"><a href="http://vision.ucsd.edu/content/pacific-labeled-corals"><img src="images/PLC_frontfig.jpg" alt="" width="180" height="180"></a>Pacific Labeled Corals is an aggregate dataset containing 5090 coral reef survey images from four Pacific monitoring projects in Moorea (French Polynesia), the northern Line Islands, Nanwan Bay (Taiwan) and Heron Reef (Australia). Pacific Labeled Corals contain a total of 251,988 expert annotations across 4 pacific reef locations, and can be used as a benchmark dataset for evaluating object recognition methods and texture descriptors as well as for domain transfer learning research. The images have all been annotated using a random point annotation tool by a coral reef expert. In addition, 200 images from each location have been cross-annotatoed by 6 experts, for a total of 7 sets of annotations for each image. The PLC dataset was published in <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130312">PLoS ONE</a>.</p>

	<h3>Moorea Labeled Corals</h3>
		<p class="margin_bottom margin_top"><a href="http://vision.ucsd.edu/content/moorea-labeled-corals"><img src="images/MLC_frontfig.jpg" alt="" width="180" height="180"></a>The Moorea Labeled Corals dataset is a subset of the Moorea Coral Reef-Long Term Ecological Research (MCR-LTER) dataset packaged for Computer Vision research. It contains over 400.000 human expert annotations on 2055 coral reef survey images from the island of Moorea in French Polynesia. Each image has 200 expert random point annotations, indicating the substrate underneath each point.</p>

		<p class="margin_bottom">The MLC dataset was published at <a href="http://vision.ucsd.edu/sites/default/files/automated_coral_annotation.pdf">CVPR 2012</a>.
		

<p class="uparrow-final border"><a href="#main">&#9650</a></p>
This website is inspired by <a href="http://www.vision.caltech.edu/welinder/">Peter Welinder</a>. Thanks Peter!
</div>

<script type="text/javascript">
$("a[href^='#']").click(function(){
	var clicked = $(this).attr("href");
	var link = $('a[data-scroll-name="' + clicked + '"]');
	
	$(".selected").attr("class","");
	
	$('body,html').animate({
		'scrollTop':   $(link).offset().top
	}, 1000, function() {
    // Animation complete.
	});
	$(this).attr("class"," selected");
});
</script>

</body>
</html>
